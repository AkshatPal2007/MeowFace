{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5445f0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Keras version: 2.10.0\n",
      "✅ GPU is available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.image import load_img, img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.layers import Dense,Conv2D,Dropout,Flatten,MaxPooling2D\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"✅ GPU is available:\", gpus)\n",
    "else:\n",
    "    print(\"❌ GPU not detected\")\n",
    "    \n",
    "Train_Dir = 'images/train'\n",
    "Test_Dir = 'images/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f0ea2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "                              image  label\n",
      "0          images/train\\angry\\0.jpg  angry\n",
      "1          images/train\\angry\\1.jpg  angry\n",
      "2         images/train\\angry\\10.jpg  angry\n",
      "3      images/train\\angry\\10002.jpg  angry\n",
      "4      images/train\\angry\\10016.jpg  angry\n",
      "...                             ...    ...\n",
      "21072     images/train\\sad\\9966.jpg    sad\n",
      "21073     images/train\\sad\\9974.jpg    sad\n",
      "21074     images/train\\sad\\9976.jpg    sad\n",
      "21075     images/train\\sad\\9986.jpg    sad\n",
      "21076     images/train\\sad\\9997.jpg    sad\n",
      "\n",
      "[21077 rows x 2 columns]\n",
      "angry completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "                            image  label\n",
      "0     images/test\\angry\\10052.jpg  angry\n",
      "1     images/test\\angry\\10065.jpg  angry\n",
      "2     images/test\\angry\\10079.jpg  angry\n",
      "3     images/test\\angry\\10095.jpg  angry\n",
      "4     images/test\\angry\\10121.jpg  angry\n",
      "...                           ...    ...\n",
      "5135     images/test\\sad\\9864.jpg    sad\n",
      "5136      images/test\\sad\\987.jpg    sad\n",
      "5137     images/test\\sad\\9885.jpg    sad\n",
      "5138     images/test\\sad\\9889.jpg    sad\n",
      "5139     images/test\\sad\\9923.jpg    sad\n",
      "\n",
      "[5140 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def createdataframe(dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dir):\n",
    "        for imagename in os.listdir(os.path.join(dir,label)):\n",
    "            image_paths.append(os.path.join(dir,label,imagename))\n",
    "            labels.append(label)\n",
    "        print(label,\"completed\")\n",
    "    return image_paths,labels\n",
    "\n",
    "train = pd.DataFrame()\n",
    "train['image'], train['label'] = createdataframe(Train_Dir)\n",
    "\n",
    "print(train)\n",
    "\n",
    "test = pd.DataFrame()\n",
    "test['image'], test['label'] = createdataframe(Test_Dir)\n",
    "\n",
    "print(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bd68139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21077/21077 [00:07<00:00, 2760.72it/s]\n",
      "100%|██████████| 5140/5140 [00:01<00:00, 2751.47it/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_features(images):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image, color_mode='grayscale', target_size=(48, 48))\n",
    "        img_array = img_to_array(img)\n",
    "        features.append(img_array)\n",
    "    features = np.array(features, dtype='float32')\n",
    "    return features\n",
    "\n",
    "train_features = extract_features(train['image'])\n",
    "test_features = extract_features(test['image'])\n",
    "\n",
    "x_train = train_features/255.0\n",
    "x_test = test_features/255.0\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(train['label'])\n",
    "\n",
    "y_train = le.transform(train['label'])\n",
    "y_test = le.transform(test['label'])\n",
    "\n",
    "y_train = to_categorical(y_train,num_classes=7)\n",
    "y_test = to_categorical(y_test,num_classes=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "842e233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e510e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "165/165 [==============================] - 8s 45ms/step - loss: 1.4380 - accuracy: 0.3061 - val_loss: 1.3670 - val_accuracy: 0.3551 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 1.3652 - accuracy: 0.3408 - val_loss: 1.3274 - val_accuracy: 0.3652 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 1.3203 - accuracy: 0.3816 - val_loss: 1.2553 - val_accuracy: 0.4292 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 1.2213 - accuracy: 0.4520 - val_loss: 1.1113 - val_accuracy: 0.5134 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 1.1550 - accuracy: 0.4918 - val_loss: 1.0592 - val_accuracy: 0.5424 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 1.0952 - accuracy: 0.5229 - val_loss: 1.0119 - val_accuracy: 0.5673 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 1.0548 - accuracy: 0.5428 - val_loss: 0.9742 - val_accuracy: 0.5809 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 1.0119 - accuracy: 0.5677 - val_loss: 0.9228 - val_accuracy: 0.6068 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.9833 - accuracy: 0.5829 - val_loss: 0.8905 - val_accuracy: 0.6263 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.9550 - accuracy: 0.5978 - val_loss: 0.8978 - val_accuracy: 0.6300 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.9279 - accuracy: 0.6074 - val_loss: 0.8863 - val_accuracy: 0.6360 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.9072 - accuracy: 0.6207 - val_loss: 0.8419 - val_accuracy: 0.6387 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.8880 - accuracy: 0.6308 - val_loss: 0.8355 - val_accuracy: 0.6533 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.8732 - accuracy: 0.6378 - val_loss: 0.8316 - val_accuracy: 0.6584 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.8542 - accuracy: 0.6479 - val_loss: 0.8182 - val_accuracy: 0.6640 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.8358 - accuracy: 0.6539 - val_loss: 0.8297 - val_accuracy: 0.6580 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.8211 - accuracy: 0.6594 - val_loss: 0.8095 - val_accuracy: 0.6689 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.8083 - accuracy: 0.6677 - val_loss: 0.8062 - val_accuracy: 0.6644 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.7954 - accuracy: 0.6777 - val_loss: 0.7987 - val_accuracy: 0.6753 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.7846 - accuracy: 0.6800 - val_loss: 0.7908 - val_accuracy: 0.6780 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.7704 - accuracy: 0.6849 - val_loss: 0.7886 - val_accuracy: 0.6823 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.7539 - accuracy: 0.6908 - val_loss: 0.7945 - val_accuracy: 0.6761 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.7437 - accuracy: 0.6969 - val_loss: 0.7939 - val_accuracy: 0.6800 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.7345 - accuracy: 0.7018 - val_loss: 0.7929 - val_accuracy: 0.6792 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.7163 - accuracy: 0.7072 - val_loss: 0.7842 - val_accuracy: 0.6804 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.7119 - accuracy: 0.7158 - val_loss: 0.7855 - val_accuracy: 0.6788 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.7024 - accuracy: 0.7175 - val_loss: 0.7936 - val_accuracy: 0.6837 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.6898 - accuracy: 0.7230 - val_loss: 0.7901 - val_accuracy: 0.6879 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.6828 - accuracy: 0.7264 - val_loss: 0.7922 - val_accuracy: 0.6772 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.6641 - accuracy: 0.7354 - val_loss: 0.7773 - val_accuracy: 0.6924 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.6620 - accuracy: 0.7335 - val_loss: 0.7889 - val_accuracy: 0.6916 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.6511 - accuracy: 0.7418 - val_loss: 0.7887 - val_accuracy: 0.6870 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.6480 - accuracy: 0.7437 - val_loss: 0.7741 - val_accuracy: 0.6907 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.6334 - accuracy: 0.7489 - val_loss: 0.7811 - val_accuracy: 0.6901 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.6259 - accuracy: 0.7502 - val_loss: 0.7802 - val_accuracy: 0.6938 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.6061 - accuracy: 0.7556 - val_loss: 0.8028 - val_accuracy: 0.6881 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.6073 - accuracy: 0.7594 - val_loss: 0.7913 - val_accuracy: 0.6942 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.6020 - accuracy: 0.7632 - val_loss: 0.7810 - val_accuracy: 0.6874 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.5559 - accuracy: 0.7801 - val_loss: 0.7836 - val_accuracy: 0.6986 - lr: 3.0000e-04\n",
      "Epoch 40/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.5249 - accuracy: 0.7926 - val_loss: 0.8016 - val_accuracy: 0.6988 - lr: 3.0000e-04\n",
      "Epoch 41/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.5194 - accuracy: 0.7937 - val_loss: 0.7894 - val_accuracy: 0.6973 - lr: 3.0000e-04\n",
      "Epoch 42/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.5139 - accuracy: 0.7982 - val_loss: 0.7916 - val_accuracy: 0.6982 - lr: 3.0000e-04\n",
      "Epoch 43/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.5027 - accuracy: 0.7986 - val_loss: 0.8020 - val_accuracy: 0.6953 - lr: 3.0000e-04\n",
      "Epoch 44/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4922 - accuracy: 0.8033 - val_loss: 0.8000 - val_accuracy: 0.6996 - lr: 9.0000e-05\n",
      "Epoch 45/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4813 - accuracy: 0.8106 - val_loss: 0.8067 - val_accuracy: 0.6992 - lr: 9.0000e-05\n",
      "Epoch 46/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4832 - accuracy: 0.8113 - val_loss: 0.8018 - val_accuracy: 0.6992 - lr: 9.0000e-05\n",
      "Epoch 47/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4740 - accuracy: 0.8134 - val_loss: 0.8085 - val_accuracy: 0.7010 - lr: 9.0000e-05\n",
      "Epoch 48/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4684 - accuracy: 0.8140 - val_loss: 0.8117 - val_accuracy: 0.7010 - lr: 9.0000e-05\n",
      "Epoch 49/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4700 - accuracy: 0.8152 - val_loss: 0.8118 - val_accuracy: 0.6977 - lr: 2.7000e-05\n",
      "Epoch 50/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4608 - accuracy: 0.8149 - val_loss: 0.8103 - val_accuracy: 0.7002 - lr: 2.7000e-05\n",
      "Epoch 51/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4648 - accuracy: 0.8163 - val_loss: 0.8101 - val_accuracy: 0.6992 - lr: 2.7000e-05\n",
      "Epoch 52/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4620 - accuracy: 0.8159 - val_loss: 0.8104 - val_accuracy: 0.7004 - lr: 2.7000e-05\n",
      "Epoch 53/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4659 - accuracy: 0.8184 - val_loss: 0.8101 - val_accuracy: 0.6982 - lr: 2.7000e-05\n",
      "Epoch 54/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4598 - accuracy: 0.8179 - val_loss: 0.8101 - val_accuracy: 0.6996 - lr: 8.1000e-06\n",
      "Epoch 55/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4536 - accuracy: 0.8204 - val_loss: 0.8108 - val_accuracy: 0.6998 - lr: 8.1000e-06\n",
      "Epoch 56/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4626 - accuracy: 0.8174 - val_loss: 0.8107 - val_accuracy: 0.6996 - lr: 8.1000e-06\n",
      "Epoch 57/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4605 - accuracy: 0.8189 - val_loss: 0.8102 - val_accuracy: 0.6990 - lr: 8.1000e-06\n",
      "Epoch 58/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4567 - accuracy: 0.8200 - val_loss: 0.8105 - val_accuracy: 0.6992 - lr: 8.1000e-06\n",
      "Epoch 59/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4568 - accuracy: 0.8223 - val_loss: 0.8106 - val_accuracy: 0.6998 - lr: 2.4300e-06\n",
      "Epoch 60/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4600 - accuracy: 0.8214 - val_loss: 0.8104 - val_accuracy: 0.7000 - lr: 2.4300e-06\n",
      "Epoch 61/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4611 - accuracy: 0.8181 - val_loss: 0.8104 - val_accuracy: 0.6994 - lr: 2.4300e-06\n",
      "Epoch 62/300\n",
      "165/165 [==============================] - 6s 38ms/step - loss: 0.4538 - accuracy: 0.8231 - val_loss: 0.8105 - val_accuracy: 0.6990 - lr: 2.4300e-06\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# convolutional layers\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# fully connected layers\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# --- Compile ---\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# --- Callbacks ---\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=15,              # Stop if no improvement in 15 epochs\n",
    "        restore_best_weights=True # Revert to best model\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.3,               # Reduce LR by 0.3x\n",
    "        patience=5,               # Wait 5 epochs before reducing LR\n",
    "        min_lr=1e-6               # Minimum learning rate\n",
    "    )\n",
    "]\n",
    "\n",
    "# --- Train ---\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=300,                   # can run safely; EarlyStopping will stop automatically\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30afd151",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"emotiondetector.json\",'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"emotiondetector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1e9af79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41c599d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(\"emotiondetector.json\",\"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"emotiondetector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a2978c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['angry','happy','neutral','sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d25e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ef(image):\n",
    "    img = load_img(image,grayscale=True)\n",
    "    feature = np.array(img)\n",
    "    feature = feature.reshape(1,48,48,1)\n",
    "    return feature/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "411eb808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of sad\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "model prediction is sad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akshat\\Desktop\\PROJECT\\Cat_Exp\\.venv\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    }
   ],
   "source": [
    "image = 'images/train/sad/3.jpg'\n",
    "print(\"original image is of sad\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is\", pred_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
